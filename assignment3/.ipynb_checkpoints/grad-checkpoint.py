{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gradient Descent optimization tools \n",
    "    \n",
    "    Steepest descent and scaled conjugate gradient \n",
    "    are currently implemented. \n",
    "    Reference codes are R version of Chuck Anderson's.\n",
    "    \n",
    "    Reference\n",
    "        Chong and Zak (2008).\n",
    "          http://www.engr.colostate.edu/~echong/book3/\n",
    "        Moller (1993).\n",
    "          http://www.sciencedirect.com/science/article/pii/S0893608005800565\n",
    "                                by lemin (Minwoo Jake Lee)\n",
    "        \n",
    "    last modified: 10/01/2011\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import copy \n",
    "import pdb\n",
    "from math import sqrt, ceil\n",
    "import sys\n",
    "import inspect\n",
    "\n",
    "\n",
    "def lineno():\n",
    "    \"\"\"Returns the current line number in our program.\"\"\"\n",
    "    #print \"::: \", inspect.currentframe().f_back.f_lineno\n",
    "    return inspect.currentframe().f_back.f_lineno\n",
    "\n",
    "###########################################################\n",
    "###\n",
    "### Scaled Conjugate Gradient \n",
    "###\n",
    "###########################################################\n",
    "\n",
    "def scg(w, gradf, optimf, *fargs, **params):\n",
    "\n",
    "    wprecision = params.pop(\"wPrecision\",1.e-8)\n",
    "    fprecision = params.pop(\"fPrecision\",1.e-8)\n",
    "    niter = params.pop(\"nIterations\",1000)\n",
    "\n",
    "    wtracep = params.pop(\"wtracep\",False)\n",
    "    ftracep = params.pop(\"ftracep\",False)\n",
    "\n",
    "    bwmin = params.pop(\"bwmin\",False)\n",
    "    wmin  = params.pop(\"wmin\",1)\n",
    "    bwmax = params.pop(\"bwmax\",False)\n",
    "    wmax  = params.pop(\"wmax\",1)\n",
    "\n",
    "    bverbose = params.pop(\"verbose\", False)\n",
    "    _beta = params.pop(\"beta\", 1e-6)\n",
    "\n",
    "    while True: # outmost loop to restart for RGD\n",
    "        nvars = len(w)\n",
    "        sigma0 = 1.0e-10\n",
    "        fold = optimf(w, *fargs)\n",
    "        fnow = fold\n",
    "        #f = fnow\n",
    "        gradnew = gradf(w, *fargs)\n",
    "        gradold = copy(gradnew)\n",
    "        d = -gradnew\t\t\t\t# Initial search direction.\n",
    "        success = True\t\t\t\t# Force calculation of directional derivs.\n",
    "        nsuccess = .0\t\t\t\t# nsuccess counts number of successes.\n",
    "        beta = _beta\t\t\t\t# Initial scale parameter.\n",
    "        betamin = 1.0e-15 \t\t\t# Lower bound on scale.\n",
    "        betamax = 1.0e5\t\t\t# Upper bound on scale.\n",
    "        #betamax = 1.0e20\t\t\t# Upper bound on scale.\n",
    "        j = 1\t\t\t\t# j counts number of iterations.\n",
    "\n",
    "        wtrace = []\n",
    "        if wtracep:\n",
    "            wtrace.append(w)\n",
    "        ftrace = []\n",
    "        if ftracep:\n",
    "            ftrace.append(fnow) #[0,0])\n",
    "        \n",
    "        while j <= niter:\n",
    "\n",
    "            ## Calculate first and second directional derivatives.\n",
    "            if success:\n",
    "                mu = np.dot(d,gradnew)\n",
    "                if bverbose and np.isnan(mu): print(\"mu is NaN\")\n",
    "                if mu >= 0:\n",
    "                    d = - gradnew\n",
    "                    mu = np.dot(d,gradnew)\n",
    "\n",
    "                kappa = np.dot(d,d)\n",
    "                if kappa < np.finfo(np.double).eps:\n",
    "                    return {'w':w, \n",
    "                            'f':fnow, \n",
    "                            'reason':\"limit on machine precision\",\n",
    "                            'wtrace':wtrace if wtracep else None, \n",
    "                            'ftrace':ftrace if ftracep else None }\n",
    "\n",
    "                sigma = sigma0/math.sqrt(kappa)\n",
    "                wplus = w + sigma * d\n",
    "                gplus = gradf(wplus, *fargs)\n",
    "                theta = np.dot(d, (gplus - gradnew))/sigma\n",
    "\n",
    "            ## Increase effective curvature and evaluate step size alpha.\n",
    "            delta = theta + beta * kappa\n",
    "            if delta is np.nan: print(\"delta is NaN\")\n",
    "            if delta <= 0.:\n",
    "                delta = beta * kappa\n",
    "                beta = beta - theta/kappa\n",
    "            \n",
    "            alpha = -mu/delta\n",
    "            #print \"alpha:\", alpha\n",
    "            ## Calculate the comparison ratio.\n",
    "            wnew = w + alpha * d\n",
    "            fnew = optimf(wnew, *fargs)\n",
    "\n",
    "            if bwmin and all(wnew <= wmin): \n",
    "                return {'w':w, \n",
    "                        'f':fnow, \n",
    "                        'reason':\"limit on w min.(%f)\" % wmin,\n",
    "                        'wtrace':wtrace if wtracep else None, \n",
    "                        'ftrace':ftrace if ftracep else None }\n",
    "            if bwmax and all(wnew >= wmax):\n",
    "                return {'w':w, \n",
    "                        'f':fnow, \n",
    "                        'reason':\"limit on w max.(%f)\" % wmax,\n",
    "                        'wtrace':wtrace if wtracep else None, \n",
    "                        'ftrace':ftrace if ftracep else None }\n",
    "\n",
    "            Delta = 2. * (fnew - fold) / (alpha*mu)\n",
    "            if not np.isnan(Delta) and Delta >= 0.:\n",
    "                success = True\n",
    "                nsuccess = nsuccess + 1\n",
    "                w = wnew\n",
    "                fnow = fnew\n",
    "                if wtracep:\n",
    "                    wtrace.append(w)\n",
    "                if ftracep:\n",
    "                    ftrace.append(fnow) #[0,0])\n",
    "            else:\n",
    "                success = False\n",
    "                fnow = fold\n",
    "            #f = fnow\n",
    "\n",
    "            #if (j % (niter /10)) ==0 and bverbose:\n",
    "            #    print \"SCG: Iteration %d  f=%f  scale=%f\" % (j, fnow, beta)\n",
    "\n",
    "            if success:\n",
    "                ## Test for termination\n",
    "                if np.max(abs(alpha*d)) < wprecision:\n",
    "                    return {'w':w, \n",
    "                            'f':fnow, \n",
    "                            'reason':\"limit on w Precision\",\n",
    "                            'wtrace':wtrace if wtracep else None, \n",
    "                            'ftrace':ftrace if ftracep else None }\n",
    "                elif np.max(abs(fnew-fold)) < fprecision:\n",
    "                    return {'w':w, \n",
    "                            'f':fnow, \n",
    "                            'reason':\"limit on f Precision\",\n",
    "                            'wtrace':wtrace if wtracep else None, \n",
    "                            'ftrace':ftrace if ftracep else None }\n",
    "                else:\n",
    "                    ## Update variables for new position\n",
    "                    fold = fnew\n",
    "                    gradold = gradnew\n",
    "                    gradnew = gradf(w, *fargs)\n",
    "                    ## If the gradient is zero then we are done.\n",
    "                    if np.dot(gradnew, gradnew) == 0:\n",
    "                        return {'w':w, \n",
    "                                'f':fnow, \n",
    "                                'reason':\"zero gradient\",\n",
    "                                'wtrace':wtrace if wtracep else None, \n",
    "                                'ftrace':ftrace if ftracep else None }\n",
    "              \n",
    "            ## Adjust beta according to comparison ratio.\n",
    "            if np.isnan(Delta) or Delta < 0.25:\n",
    "                beta = min(4.0*beta, betamax)\n",
    "            elif Delta > 0.75:\n",
    "                beta = max(0.5*beta, betamin)\n",
    "\n",
    "            ## Update search direction using Polak-Ribiere formula, or re-start \n",
    "            ## in direction of negative gradient after nparams steps.\n",
    "\n",
    "            if nsuccess == nvars:\n",
    "                d = -gradnew\n",
    "                nsuccess = 0\n",
    "            elif success:\n",
    "                gamma = np.dot(gradold - gradnew, gradnew / mu)\n",
    "                d = gamma * d - gradnew\n",
    "\n",
    "            j = j + 1\n",
    "\n",
    "        return {'w':w, \n",
    "                'f':fnow, \n",
    "                'reason':\"reached limit of nIterations\",\n",
    "                'wtrace':wtrace if wtracep else None, \n",
    "                'ftrace':ftrace if ftracep else None }\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "### Steepest descent\n",
    "    \n",
    "\n",
    "floatPrecision = sys.float_info.epsilon\n",
    "\n",
    "def steepest(x,gradf, f, *fargs, **params):\n",
    "    \"\"\"steepest:\n",
    "    Example:\n",
    "    def parabola(x,xmin,s):\n",
    "        d = x - xmin\n",
    "        return np.dot( np.dot(d.T, s), d)\n",
    "    def parabolaGrad(x,xmin,s):\n",
    "        d = x - xmin\n",
    "        return 2 * np.dot(s, d)\n",
    "    center = np.array([5,5])\n",
    "    S = np.array([[5,4],[4,5]])\n",
    "    firstx = np.array([-1.0,2.0])\n",
    "    r = steepest(firstx, parabola, parabolaGrad, center, S,\n",
    "                 stepsize=0.01,xPrecision=0.001, nIterations=1000)\n",
    "    print('Optimal: point',r[0],'f',r[1])\"\"\"\n",
    "\n",
    "    stepsize= params.pop(\"stepsize\",0.1)\n",
    "    evalFunc = params.pop(\"evalFunc\",lambda x: \"Eval \"+str(x))\n",
    "    nIterations = params.pop(\"nIterations\",1000)\n",
    "    xPrecision = params.pop(\"xPrecision\",1.e-8)\n",
    "    fPrecision = params.pop(\"fPrecision\",1.e-8)\n",
    "    wtracep = params.pop(\"wtracep\",False)\n",
    "    ftracep = params.pop(\"ftracep\",False)\n",
    "\n",
    "    i = 1\n",
    "    if wtracep:\n",
    "        wtrace = np.zeros((nIterations+1,len(x)))\n",
    "        wtrace[0,:] = x\n",
    "    else:\n",
    "        wtrace = None\n",
    "    oldf = f(x,*fargs)\n",
    "    if ftracep:\n",
    "        ftrace = [0] * (nIterations+1) #np.zeros(nIterations+1)\n",
    "        ftrace[0] = f(x,*fargs)# [0,0]\n",
    "    else:\n",
    "        ftrace = None\n",
    "  \n",
    "    while i <= nIterations:\n",
    "        g = gradf(x,*fargs)\n",
    "        newx = x - stepsize * g\n",
    "        newf = f(newx,*fargs)\n",
    "        #if i % max(1,(nIterations/10)) == 0:\n",
    "        #    print \"Steepest: Iteration\",i,\"Error\",evalFunc(newf)\n",
    "        if wtracep:\n",
    "            wtrace[i,:] = newx\n",
    "        if ftracep:\n",
    "            ftrace[i] = newf #[0,0]\n",
    "\n",
    "        if np.any(newx == np.nan) or newf == np.nan:\n",
    "            raise ValueError(\"Error: Steepest descent produced newx that is NaN. Stepsize may be too large.\")\n",
    "        if np.any(np.isinf(newx)) or  np.isinf(newf):\n",
    "            raise ValueError(\"Error: Steepest descent produced newx that is NaN. Stepsize may be too large.\")\n",
    "        if max(abs(newx - x)) < xPrecision:\n",
    "            return {'w':newx, 'f':newf, 'nIterations':i, 'wtrace':wtrace[:i,:] if wtracep else None, 'ftrace':ftrace[:i] if ftracep else None,\n",
    "                    'reason':\"limit on x precision\"}\n",
    "        if abs(newf - oldf) < fPrecision:\n",
    "            return {'w':newx, 'f':newf, 'nIterations':i, 'wtrace':wtrace[:i,:] if wtracep else None, 'ftrace':ftrace[:i] if ftracep else None,\n",
    "                    'reason':\"limit on f precision\"}\n",
    "        x = newx\n",
    "        oldf = newf\n",
    "        i += 1\n",
    "\n",
    "    return {'w':newx, 'f':newf, 'nIterations':i, 'wtrace':wtrace[:i,:] if wtracep else None, 'ftrace':ftrace[:i] if ftracep else None, 'reason':\"did not converge\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
